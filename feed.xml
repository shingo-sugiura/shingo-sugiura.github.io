<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://shingo-sugiura.github.io/</id><title>Cozy World Blog</title><subtitle>Blog</subtitle> <updated>2023-09-23T13:54:29+09:00</updated> <author> <name>Shingo Sugiura</name> <uri>https://shingo-sugiura.github.io/</uri> </author><link rel="self" type="application/atom+xml" href="https://shingo-sugiura.github.io/feed.xml"/><link rel="alternate" type="text/html" hreflang="en" href="https://shingo-sugiura.github.io/"/> <generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator> <rights> © 2023 Shingo Sugiura </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>Sampling Importance Resampling</title><link href="https://shingo-sugiura.github.io/posts/sir/" rel="alternate" type="text/html" title="Sampling Importance Resampling" /><published>2023-08-11T10:00:00+09:00</published> <updated>2023-08-19T00:22:09+09:00</updated> <id>https://shingo-sugiura.github.io/posts/sir/</id> <content src="https://shingo-sugiura.github.io/posts/sir/" /> <author> <name>Shingo Sugiura</name> </author> <category term="Math" /> <summary> Sampling Importance Resampling(간단하게 Importance Resampling)은 임의의 분포로부터 샘플을 생성하는 알고리즘이다. 어떤 복잡한 분포든 evaluation만 가능하다면 그 분포에 근사하여 샘플을 생성할 수 있다. 정규화 되지 않은(unnormalized) 분포 $ \hat p $ 와 정규화된 분포(pdf) $p$를 생각해 보자. \[\frac{\hat p}{C} = p\] \[C = \int_{\Omega} \hat p(x) dx\] 앞으로 hat표시가 있는 $\hat p$ 는 정규화 되지 않은 분포, $p$는 정규화된 분포(pdf)로 구분해서 표기하겠다. 우리가 샘플링 하고싶은 target 분포는 정규화 되지 않은 $\hat p(x)$ 이다. 알고리즘... </summary> </entry> <entry><title>Multiple Importance Sampling</title><link href="https://shingo-sugiura.github.io/posts/mis/" rel="alternate" type="text/html" title="Multiple Importance Sampling" /><published>2023-08-08T02:00:00+09:00</published> <updated>2023-09-22T23:31:04+09:00</updated> <id>https://shingo-sugiura.github.io/posts/mis/</id> <content src="https://shingo-sugiura.github.io/posts/mis/" /> <author> <name>Shingo Sugiura</name> </author> <category term="Math" /> <summary> Multi-sample Estimator 바로 이전 포스트에서 Monte Carlo Estimator에 대해 알아봤다. \[F_N = \frac{1}{N} \sum_{i=1}^{N} \frac{f(X_i)}{p(X_i)}\] Estimator의 기댓값은 구하고자 하는 적분값이 되고, 이 결과는 편향이 없는(unbiased) 정확한 방법이다. \[E[F_N] = \int_{\Omega} f(x) dx = I\] 이 Unbiased estimator 여러개에 대해 $\sum_{i} w_i = 1$ 을 만족하는 weighted sum을 생각해 본다면 \[\sum_{i=1}^{n} w_i E[F_i] = \sum_{i=1}^{n} \frac{1}{n_i} \sum_{j=1}^{n_i} w_i E\B... </summary> </entry> <entry><title>Monte Carlo Estimator</title><link href="https://shingo-sugiura.github.io/posts/mce/" rel="alternate" type="text/html" title="Monte Carlo Estimator" /><published>2023-08-02T23:00:00+09:00</published> <updated>2023-08-09T05:45:42+09:00</updated> <id>https://shingo-sugiura.github.io/posts/mce/</id> <content src="https://shingo-sugiura.github.io/posts/mce/" /> <author> <name>Shingo Sugiura</name> </author> <category term="Math" /> <summary> Light Transport Simulation의 기본이 되는 Monte Carlo Estimator와 분산을 효과적으로 줄이는 기법인 Importance Sampling에 대해 알아보자. 기댓값과 분산 기댓값 Expected Value 확률론에서 확률 변수의 기댓값이란 어떤 랜덤 프로세스를 반복했을 때 기대할 수 있는 값의 평균이다. 확률 변수 $X$의 기댓값은 $E[X]$로 쓴다. 이산 확률 변수의 기댓값은 다음과 같이, \[E[X]_{X\sim{p}} = \sum_{i=1}^{n} {x_i} \cdot p(x_i)\] 연속 활률 변수의 기댓값은 다음과 같이 정의된다. \[E[X]_{X\sim{p}} = \int_{\Omega} x \cdot p(x) dx\] 예시 주사위 던지기는 랜... </summary> </entry> <entry><title>Weighted Reservoir Sampling</title><link href="https://shingo-sugiura.github.io/posts/wrs/" rel="alternate" type="text/html" title="Weighted Reservoir Sampling" /><published>2023-07-31T23:00:00+09:00</published> <updated>2023-08-08T10:50:18+09:00</updated> <id>https://shingo-sugiura.github.io/posts/wrs/</id> <content src="https://shingo-sugiura.github.io/posts/wrs/" /> <author> <name>Shingo Sugiura</name> </author> <category term="Math" /> <summary> 문제 설명 어떤 데이터 $ E_i \, (1 \le i \le N) $ 스트림이 있는데 시점 $t$ 에 $E_t$ 를 볼 수 있고 스트림의 총 길이 $N$ 은 미리 알 수 없다. 각 $E_i$ 에 가중치 $w_i$ 가 주어지고 우리는 가중치에 비례해 데이터를 샘플링 하고 싶다. 즉, 총 가중치 $ W = \sum_{k \le N} w_k $ 로 모든 데이터를 본 시점에 랜덤한 샘플 $ E_j $ 를 $ w_j / W $ 의 확률로 뽑고자 한다. Weighted Reservoir Sampling Weighted Reservoir Sampling 알고리즘은 현재 선택된 샘플 $R$ 과 가중치 합 $w_{sum}$ 을 저장하는 Reservoir $\mathcal{R} = \{ R,\,w_{sum} \... </summary> </entry> <entry><title>Quaternion differentiation</title><link href="https://shingo-sugiura.github.io/posts/quaternion-integration/" rel="alternate" type="text/html" title="Quaternion differentiation" /><published>2023-02-27T22:00:00+09:00</published> <updated>2023-06-26T20:59:35+09:00</updated> <id>https://shingo-sugiura.github.io/posts/quaternion-integration/</id> <content src="https://shingo-sugiura.github.io/posts/quaternion-integration/" /> <author> <name>Shingo Sugiura</name> </author> <category term="Math" /> <summary> [!] 이 문서에서 사용되는 \vec 표기는 벡터가 아니라 pure quaternion입니다. 예를 들어 $\vec w$는 $ 0 + w_x i + w_y j + w_z k $ 형태의 사원수. 또한 \hat은 표기는 unit quaternion 입니다. [!] 아래 유도 과정을 이해 하기 위해선 사원수에 대한 이해가 필요합니다. Intro \[\hat q_{new} = \hat q_{old} + \frac 1 2 \vec w \hat q_{old} \Delta t\] Angular velocity vector를 사원수 orientation에 적용(수치 적분) 하는 방법을 찾다가 위와 같은 식 발견했는데 많은 사람들이 유도 과정에 대한 이해는 생략하고 black box formula 처럼 사용하는... </summary> </entry> </feed>
